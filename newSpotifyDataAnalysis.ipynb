{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Importing Libraries](#importing-libraries)\n",
    "2. [Data Description](#data-description)\n",
    "    * [Multiple Genres](#multiple-genres)\n",
    "    * [Visualisation](#visualisation)\n",
    "3. [Data Preprocessing](#data-cleaning)\n",
    "4. [Data Modelling](#data-modelling)\n",
    "    * [Neural Network](#neural-network)\n",
    "    * [K-Nearest Neighbours](#k-nearest-neighbours)\n",
    "    * [Random Forest](#random-forest)\n",
    "    * [Model Comparison](#model-comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries <a class=\"anchor\" id=\"importing-libraries\"></a>\n",
    "\n",
    "Here, we import all the necessary libraries for our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import layers, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description <a class=\"anchor\" id=\"data-description\"></a>\n",
    "\n",
    "First, the data is loaded and basic information about the data is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('csvs/dataset.csv', index_col=0)\n",
    "\n",
    "tracks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to identify and predict the genres of the song, so we try display and see how many genres are there in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of genres: {}'.format(tracks.track_genre.nunique()))\n",
    "\n",
    "# Get a count of all genre\n",
    "tracks.track_genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Genres <a class=\"anchor\" id=\"multiple-genres\"></a>\n",
    " \n",
    "We discover that some song may have multiple genres. To improve our modelling, we will be using tracks with one genre only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by popularity first, so when we drop duplicate we drop lower popularity\n",
    "# Drop duplicate if track_name, duration_ms, artists and track_genre are all the same\n",
    "tracks.sort_values(by=['popularity'],ascending=False,inplace=True)\n",
    "tracks.drop_duplicates(subset=['track_name','duration_ms','artists','track_genre'],inplace=True)\n",
    "\n",
    "# If track_name, duration_ms and artists are same, but genre is different, aggregate the genre\n",
    "tracks = tracks.groupby(['track_name','duration_ms','artists'],as_index=False).agg({'track_genre':lambda x: ','.join(x),\n",
    "                                                                                                  'album_name': 'first',\n",
    "                                                                                                  'track_id': 'first',\n",
    "                                                                                                  'popularity': 'max',\n",
    "                                                                                                  'explicit': 'first',\n",
    "                                                                                                  'danceability': 'first',\n",
    "                                                                                                  'energy': 'first',\n",
    "                                                                                                  'loudness': 'first',\n",
    "                                                                                                  'speechiness': 'first',\n",
    "                                                                                                  'acousticness': 'first',\n",
    "                                                                                                  'instrumentalness': 'first',\n",
    "                                                                                                  'liveness': 'first',\n",
    "                                                                                                  'valence': 'first',\n",
    "                                                                                                  'tempo': 'first',\n",
    "                                                                                                  'key': 'first',\n",
    "                                                                                                  'mode': 'first',\n",
    "                                                                                                  'time_signature': 'first'})\n",
    "\n",
    "\n",
    "# Remove all tracks with more than one genre\n",
    "tracks = tracks[tracks['track_genre'].str.contains(',') == False]\n",
    "tracks.track_genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, any genre with less than 500 tracks does not constitute enough training and test sample, and will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all genres with less than 500 tracks, maintain all columns\n",
    "tracks = tracks.groupby('track_genre').filter(lambda x: len(x) > 500)\n",
    "tracks.track_genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our modelling easier, we will limit our selection to a hand selected few genres. As much as the top 10 genre present an interesting opportunity, a cursory glance at the data shows that the top 10 genres are not very distinct from each other. Hence, we will select a few genres that are more significantly distinct from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_popularity = tracks.groupby('track_genre')['popularity'].mean()\n",
    "genre_popularity.sort_values(ascending=False)\n",
    "\n",
    "# What is the difference between pop-film, k-pop, pop? \n",
    "# And what is the difference between sad and emo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following genre for our modelling, and remove the rest of the genres from the dataset.\n",
    "- Country\n",
    "- Chill\n",
    "- K-Pop\n",
    "- Club\n",
    "- Rock-n-Roll\n",
    "- Classical\n",
    "- Sleep\n",
    "- Electronic\n",
    "- Ambient\n",
    "- Opera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the genres listed above\n",
    "tracks = tracks[tracks['track_genre'].isin(['country', 'chill', 'k-pop', 'club', 'rock-n-roll', 'classical', 'sleep', 'electronic', 'ambient', 'opera'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation <a class=\"anchor\" id=\"visualisation\"></a>\n",
    "\n",
    "Since we have our genres sorted, we can now visualise the data to see if there are any interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all features that are not numerical\n",
    "feature_numerical = [feature for feature in tracks.columns if tracks[feature].dtype!='O']\n",
    "\n",
    "# If a feature has less than 50 unique values, it is considered as discrete\n",
    "discrete_features = [feature for feature in feature_numerical if tracks[feature].nunique()<50]\n",
    "continuous_features = [feature for feature in feature_numerical if feature not in discrete_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first plot grouped barcharts to see the incidence of each discrete numerical feature corresponding to each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in discrete_features:\n",
    "    g = sns.catplot(\n",
    "        data=tracks, kind=\"count\",\n",
    "        x=\"track_genre\", hue=feature,\n",
    "        estimator=np.median, palette=\"hls\"\n",
    "    )\n",
    "\n",
    "    g.set_axis_labels(\"Genre\", \"Count\")\n",
    "    g.set_xticklabels(rotation=45)\n",
    "    g.legend.set_title(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot grouped boxenplots to see the distribution of each continuous numerical feature corresponding to each genre. This helps us to identify outliers and the statistical distribution of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in continuous_features:\n",
    "    g = sns.boxenplot(\n",
    "        data=tracks, x=\"track_genre\", y=feature,\n",
    "        flier_kws=dict(linewidths=0, s=10)\n",
    "    )\n",
    "\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "    g.set(title=feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also checking the density of each continuous numerical feature corresponding to each genre. This helps us visualise which genre dominates which range of values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in continuous_features:\n",
    "    g = sns.displot(\n",
    "        data=tracks, x=feature, hue=\"track_genre\", \n",
    "        multiple=\"fill\", kind=\"kde\", bw_adjust=2,\n",
    "        linewidth=0.5, clip=(tracks[feature].min(), tracks[feature].max())\n",
    "    )\n",
    "\n",
    "    g.set(title=feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the histogram density of each discrete numerical feature to see if there are any interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in discrete_features:\n",
    "    g = sns.displot(\n",
    "        data=tracks, x=feature, hue=\"track_genre\", \n",
    "        linewidth=0.5, multiple=\"fill\",\n",
    "        discrete=True\n",
    "    )\n",
    "\n",
    "    g.set(title=feature, ylabel=\"Proportion\", \\\n",
    "        xticks=range(0, tracks[feature].max() + 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing <a class=\"anchor\" id=\"data-cleaning\"></a>\n",
    "\n",
    "We start off with basic data cleaning, removing null data and removing unnecessary columns according to our EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row where track_name = null\n",
    "tracks.drop(tracks.index[tracks['track_name'].isnull()], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also remove Track ID from our dataset as the ID is randomly generated data. Additionally, track name, artist name and album name will be removed as well. These three category are too diverse and will be hard to generalize, even if they provide very useful information. \n",
    "\n",
    "We will also drop the track key, as it will present too many dimension for our model to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the track_id column\n",
    "tracks.drop('track_id', axis=1, inplace=True)\n",
    "\n",
    "# Drop the track_name, artists, album_name columns\n",
    "tracks.drop(['track_name', 'artists', 'album_name'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the key column\n",
    "tracks.drop('key', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will discretize both loudness, tempo and duration_ms into 10 bins each. The exact value of these columns are not important, but their rough bins will help better inform the model.\n",
    "\n",
    "We will also normalise the popularity columns, as they are on a different scale from the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the loudness column into 10 bins, normalised within 0 and 1\n",
    "tracks['loudness'] = pd.cut(tracks['loudness'], 10, labels=False)\n",
    "tracks['loudness'] = MinMaxScaler().fit_transform(tracks[['loudness']])\n",
    "\n",
    "# Discretize the tempo column into 10 bins, normalised within 0 and 1\n",
    "tracks['tempo'] = pd.cut(tracks['tempo'], 10, labels=False)\n",
    "tracks['tempo'] = MinMaxScaler().fit_transform(tracks[['tempo']])\n",
    "\n",
    "# Normalise the duration_ms column through the use of log transformation, then normalise within 0 and 1\n",
    "tracks['duration_ms'] = np.log(tracks['duration_ms'])\n",
    "tracks['duration_ms'] = MinMaxScaler().fit_transform(tracks[['duration_ms']])\n",
    "\n",
    "# Normalise the popularity column through MinMaxScaler\n",
    "tracks['popularity'] = MinMaxScaler().fit_transform(tracks[['popularity']])\n",
    "\n",
    "# Normalise the time_signature column through MinMaxScaler\n",
    "tracks['time_signature'] = MinMaxScaler().fit_transform(tracks[['time_signature']])\n",
    "\n",
    "# Describe the dataset\n",
    "tracks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make sure each of the genres has 500 sample exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop individual rows until the number of tracks per genre is equal\n",
    "tracks = tracks.groupby('track_genre').apply(lambda x: x.sample(tracks.track_genre.value_counts().min(), random_state=42).reset_index(drop=True))\n",
    "tracks.track_genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we output the data into a csv file for possible reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.to_csv('csvs/clean_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling <a class=\"anchor\" id=\"data-modelling\"></a>\n",
    "\n",
    "We split the data into training and test set, with 80% of the data going into training and 20% going into test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tracks.drop('track_genre', axis=1)\n",
    "y = tracks['track_genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "X_test = np.asarray(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network <a class=\"anchor\" id=\"neural-network\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use keras_tuner to help carry out hyperparameter tuning for our neural network. We vary the amount of neuron per layer, and try dropout layer to help reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            units=hp.Int('units_1st', min_value=16, max_value=256, step=8),\n",
    "            activation='relu',\n",
    "            input_shape=(X_train.shape[1],)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if hp.Boolean(\"dropout_1st\"):\n",
    "        model.add(\n",
    "            layers.Dropout(\n",
    "                rate=hp.Float('dropout_rate_1st', min_value=0.1, max_value=0.5, step=0.05)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            units=hp.Int('units_2nd', min_value=16, max_value=256, step=8),\n",
    "            activation='relu'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if hp.Boolean(\"dropout_2nd\"):\n",
    "        model.add(\n",
    "            layers.Dropout(\n",
    "                rate=hp.Float('dropout_rate_2nd', min_value=0.1, max_value=0.5, step=0.05)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    model.add(layers.Dense(units=10,activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=[\n",
    "                    metrics.CategoricalAccuracy(name='cat_acc'),\n",
    "                    'accuracy'\n",
    "                ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of grid search, we use Hyperband, which is type of random search. This is because grid search is very computationally expensive, and Hyperband is a more efficient way to carry out random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "lbl.fit(y_train)\n",
    "ann_y_train = lbl.transform(y_train)\n",
    "ann_y_test = lbl.transform(y_test)\n",
    "\n",
    "ann_y_train = tf.keras.utils.to_categorical(ann_y_train)\n",
    "ann_y_test = tf.keras.utils.to_categorical(ann_y_test)\n",
    "\n",
    "build_model(kt.HyperParameters())\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    seed=42,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True, # change to False if you want to resume a previous search\n",
    "    directory='search',\n",
    "    project_name='spotify'\n",
    ")\n",
    "    \n",
    "tuner.search(X_train, ann_y_train, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the best hyperparameter from the tuner and use it to train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=3)\n",
    "ann_model = build_model(best_hps[0])\n",
    "\n",
    "plot_model(ann_model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "\n",
    "ann_history = ann_model.fit(X_train, ann_y_train, epochs=120, batch_size=16, validation_split=0.1)\n",
    "ann_score = ann_model.evaluate(X_test, ann_y_test)\n",
    "ann_cm = confusion_matrix(y_test, lbl.inverse_transform(ann_model.predict(X_test)))\n",
    "ann_cr = classification_report(y_test, lbl.inverse_transform(ann_model.predict(X_test)))\n",
    "\n",
    "print('Accuracy: {}'.format(ann_score[1]))\n",
    "print('Confusion Matrix: \\n{}'.format(ann_cm))\n",
    "print('Classification Report: \\n{}'.format(ann_cr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours <a class=\"anchor\" id=\"k-nearest-neighbours\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours is a simple algorithm that uses the distance between the data points to classify the data. We use the euclidean distance to calculate the distance between the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],   \n",
    "}\n",
    "\n",
    "knn_grid = GridSearchCV(knn, knn_param_grid, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "knn_grid.fit(X_train, y_train)\n",
    "print('Best KNN Params: {}'.format(knn_grid.best_params_))\n",
    "\n",
    "knn_pred = knn_grid.predict(X_test)\n",
    "\n",
    "# Best param was {'leaf_size': 10, 'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}\n",
    "# Run the following to prevent GridSearchCV from running again\n",
    "# knn = KNeighborsClassifier(leaf_size=10, metric='manhattan', n_neighbors=7, weights='distance')\n",
    "# knn.fit(X_train, y_train)\n",
    "# knn_pred = knn.predict(X_test)\n",
    "\n",
    "knn_score = accuracy_score(y_test, knn_pred)\n",
    "knn_cm = confusion_matrix(y_test, knn_pred)\n",
    "knn_cr = classification_report(y_test, knn_pred)\n",
    "\n",
    "print('KNN Accuracy: {}'.format(knn_score))\n",
    "print('KNN Confusion Matrix: \\n{}'.format(knn_cm))\n",
    "print('KNN Classification Report: \\n{}'.format(knn_cr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest <a class=\"anchor\" id=\"random-forest\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a random forest model with 100 trees, and use the model to predict the genre of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 7, 9, 11, 13, 15],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "}\n",
    "\n",
    "rfc_grid = GridSearchCV(rfc, rfc_param_grid, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "rfc_grid.fit(X_train, y_train)\n",
    "print('Best RFC Params: {}'.format(rfc_grid.best_params_))\n",
    "\n",
    "rfc_pred = rfc_grid.predict(X_test)\n",
    "\n",
    "# Best param was {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 200}\n",
    "# Run the following to prevent GridSearchCV from running again\n",
    "# rfc = RandomForestClassifier(max_depth=15, max_features='sqrt', n_estimators=200, random_state=42)\n",
    "# rfc.fit(X_train, y_train)\n",
    "# rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "rfc_cr = classification_report(y_test, rfc_pred)\n",
    "rfc_cm = confusion_matrix(y_test, rfc_pred)\n",
    "rfc_score = accuracy_score(y_test, rfc_pred)\n",
    "\n",
    "print('RFC Accuracy: {}'.format(rfc_score))\n",
    "print('RFC Confusion Matrix: \\n{}'.format(rfc_cm))\n",
    "print('RFC Classification Report: \\n{}'.format(rfc_cr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison <a class=\"anchor\" id=\"model-comparison\"></a>\n",
    "\n",
    "Here, we compare the classification report of the three models. We use some graphs to help visualise the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(model, X_test, y_test):\n",
    "    cm = ConfusionMatrixDisplay.from_estimator(\n",
    "        model, X_test, y_test, xticks_rotation=45\n",
    "    )\n",
    "    cm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(ann_cm, ann_grid)\n",
    "plot_cm(rfc_cm, rfc_grid)\n",
    "plot_cm(knn_grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the accuracy, loss, F1, and precision-recall curve of the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy of the models\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ann_history.history['accuracy'], label='ANN')\n",
    "plt.plot(ann_history.history['val_accuracy'], label='ANN Validation')\n",
    "plt.axhline(y=rfc_score, color='r', linestyle='-', label='RFC')\n",
    "plt.axhline(y=knn_score, color='g', linestyle='-', label='KNN')\n",
    "plt.title('Accuracy of Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss of the models\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ann_history.history['loss'], label='ANN')\n",
    "plt.plot(ann_history.history['val_loss'], label='ANN Validation')\n",
    "plt.title('Loss of Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
